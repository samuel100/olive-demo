{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ«’ Olive Tutorial: Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤— Login to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Fine-tune with Olive\n",
    "\n",
    "In this tutorial we'll fine tune the Phi-3.5-mini-instruct model for the task of phrase classification i.e. given a phrase the model will classify into one of joy/surprise/fear/sadness. The dataset, which is available on Hugging Face, is in the following format:\n",
    "\n",
    "```json\n",
    "{\"phrase\": \"I'm thrilled to start my new job!\", \"tone\": \"joy\"}\n",
    "{\"phrase\": \"I can't believe I lost my keys again.\", \"tone\": \"surprise\"}\n",
    "{\"phrase\": \"This haunted house is terrifying!\", \"tone\": \"fear\"}\n",
    "{\"phrase\": \"Winning the lottery is a dream come true.\", \"tone\": \"joy\"}\n",
    "{\"phrase\": \"Missing the concert is really disappointing.\", \"tone\": \"sadness\"}\n",
    "```\n",
    "\n",
    "To fine-tune you only need to enter a few arguments into the `olive finetune` command:\n",
    "\n",
    "- `--method` the method used for fine-tuning. `lora` and `qlora` are supported.\n",
    "- `--data_name` the Hugging Face dataset name.\n",
    "- `--text-template` the template to generate text field from. E.g. â€˜### Question: {prompt} n### Answer: {response}â€™. For Phi-3, the chat format is `<|user|>\\n{prompt}<|end|>\\n<|assistant|>\\n{response}<|end|>`\n",
    "- `--model_name_or_path` The model checkpoint for weights initialization. This can be a Hugging Face model repo, a local path, or an Azure AI Model registry.\n",
    "\n",
    "More details on available options can be found [here](https://microsoft.github.io/Olive/features/cli.html#finetune).\n",
    "\n",
    "### ðŸ§  Supported models\n",
    "\n",
    "Whilst Olive can fine-tune any PyTorch model through a user-provided `io_config` (type, shape etc.,). However, the most popular models are supported out-of-the-box such as:\n",
    "\n",
    "- Phi\n",
    "- Llama\n",
    "- Mistral\n",
    "- Gemma\n",
    "- Qwen\n",
    "\n",
    "For more details on supported *architectures*, read [Hugging Face Optimum Overview](https://huggingface.co/docs/optimum/en/exporters/onnx/overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# It can take around 20-30mins for the finetuning to complete.\n",
    "!olive finetune \\\n",
    "    --method qlora \\\n",
    "    --model_name_or_path microsoft/Phi-3.5-mini-instruct \\\n",
    "    --trust_remote_code \\\n",
    "    --use_ort_genai \\\n",
    "    --data_name xxyyzzz/phrase_classification \\\n",
    "    --text_template \"<|user|>\\n{phrase}<|end|>\\n<|assistant|>\\n{tone}<|end|>\" \\\n",
    "    --max_steps 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ¨ Test the model using ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime_genai as og\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "model_folder = \"optimized-model\"\n",
    "\n",
    "model = og.Model(model_folder)\n",
    "tokenizer = og.Tokenizer(model)\n",
    "tokenizer_stream = tokenizer.create_stream()\n",
    "\n",
    "weights_file = os.path.join(model_folder, \"adapter_weights.npz\") \n",
    "adapter_weights = np.load(weights_file)\n",
    " \n",
    "# Set the max length to something sensible by default,\n",
    "# since otherwise it will be set to the entire context length\n",
    "search_options = {}\n",
    "search_options['max_length'] = 200\n",
    "search_options['past_present_share_buffer'] = False\n",
    "\n",
    "chat_template = \"<|user|>\\n{input}<|end|>\\n<|assistant|>\"\n",
    "\n",
    "text = input(\"Input: \")\n",
    "if not text:\n",
    "   print(\"Error, input cannot be empty\")\n",
    "   exit\n",
    "\n",
    "prompt = f'{chat_template.format(input=text)}'\n",
    "\n",
    "input_tokens = tokenizer.encode(prompt)\n",
    "\n",
    "params = og.GeneratorParams(model)\n",
    "for key in adapter_weights.keys():\n",
    "    params.set_model_input(key, adapter_weights[key])\n",
    "params.set_search_options(**search_options)\n",
    "params.input_ids = input_tokens\n",
    "generator = og.Generator(model, params)\n",
    "\n",
    "\n",
    "print(\"Output: \", end='', flush=True)\n",
    "\n",
    "try:\n",
    "   while not generator.is_done():\n",
    "     generator.compute_logits()\n",
    "     generator.generate_next_token()\n",
    "\n",
    "     new_token = generator.get_next_tokens()[0]\n",
    "     print(tokenizer_stream.decode(new_token), end='', flush=True)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"  --control+c pressed, aborting generation--\")\n",
    "\n",
    "print()\n",
    "# free up resources\n",
    "del generator\n",
    "del model\n",
    "del tokenizer\n",
    "del tokenizer_stream"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olive-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
